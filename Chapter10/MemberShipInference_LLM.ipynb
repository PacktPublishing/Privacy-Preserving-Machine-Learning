{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Define a simple Variational Autoencoder (VAE) as the generative model\nclass VAE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim * 2)  # Two times latent_dim for mean and log-variance\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n    \n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def forward(self, x):\n        x = self.encoder(x)\n        mu, log_var = x[:, :latent_dim], x[:, latent_dim:]\n        z = self.reparameterize(mu, log_var)\n        reconstructed = self.decoder(z)\n        return reconstructed, mu, log_var\n\n# Generate synthetic data for demonstration (replace with your actual dataset)\nnum_samples = 1000\ndata_dim = 20\ndata = torch.tensor(np.random.randint(2, size=(num_samples, data_dim)), dtype=torch.float32)\n\nprint(data)\n\n# Initialize the VAE model\ninput_dim = data_dim\nhidden_dim = 64\nlatent_dim = 16\nvae = VAE(input_dim, hidden_dim, latent_dim)\n\n# Define an adversary model (a simple feedforward neural network)\nclass Adversary(nn.Module):\n    def __init__(self, input_dim):\n        super(Adversary, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# Train the VAE model (replace with your actual training process)\n\n# Train the adversary model\nadversary = Adversary(latent_dim)\noptimizer = optim.Adam(adversary.parameters(), lr=0.001)\ncriterion = nn.BCELoss()\n\n# Prepare target data for the membership inference attack\ntarget_data_point = torch.tensor(np.random.randint(2, size=data_dim), dtype=torch.float32)\n\n# Membership inference attack function\ndef membership_inference_attack(vae, adversary, target_data_point):\n    # Encode the target data point using the VAE\n    with torch.no_grad():\n        target_data_point = target_data_point.unsqueeze(0)  # Add batch dimension\n        reconstructed, mu, log_var = vae(target_data_point)\n    \n    # Use the adversary to predict membership\n    prediction = adversary(mu)\n    \n    # If the prediction is close to 1, the target data point is likely a member\n    if prediction.item() > 0.5:\n        return \"Member\"\n    else:\n        return \"Non-Member\"\n\n# Perform the membership inference attack\nresult = membership_inference_attack(vae, adversary, target_data_point)\n\n# Output the result\nprint(\"Membership Inference Result:\", result)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:50:14.646560Z","iopub.execute_input":"2023-09-14T08:50:14.647249Z","iopub.status.idle":"2023-09-14T08:50:19.123346Z","shell.execute_reply.started":"2023-09-14T08:50:14.647211Z","shell.execute_reply":"2023-09-14T08:50:19.122099Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"tensor([[1., 1., 0.,  ..., 1., 1., 1.],\n        [1., 0., 1.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 1., 0., 1.],\n        ...,\n        [1., 0., 1.,  ..., 1., 1., 1.],\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        [1., 1., 0.,  ..., 1., 0., 0.]])\nMembership Inference Result: Member\n","output_type":"stream"}]}]}